#!/usr/bin/env python3
# =====================================================================
#  onnx2int8.py  ―  Tiny-ViT INT8 packer (weights.bin + model.h only)
#  Generates:
#      • weights.bin   – INT8/INT32 tensors (weights, biases, LN γ/β)
#      • model.h       – dimensions, fixed-point shifts, arena layout,
#                        weight offsets and pre-computed Q0.15 scalers
#  Runtime becomes 100 % integer; no floating-point tables are loaded.
# ---------------------------------------------------------------------
#  Copyright (c) 2025  Alessandro Varaldi and contributors
#  MIT License
# =====================================================================

import argparse
import math
from pathlib import Path
from typing import Dict, Tuple

import numpy as np
import onnx
import yaml

# ---------------------------------------------------------------------
#  Quantisation helpers
# ---------------------------------------------------------------------
def sym_int8_quant(arr: np.ndarray, *, per_channel: bool, axis: int = 0
                   ) -> Tuple[np.ndarray, np.ndarray]:
    """
    Symmetric INT8 quantisation.
    Returns (q_int8, scale) where `scale` is either a scalar or a
    float32 vector (per-channel).
    """
    if per_channel:
        arr_ch_first = np.moveaxis(arr, axis, 0)            # (C, …)
        flat         = arr_ch_first.reshape(arr_ch_first.shape[0], -1)
        scales       = np.abs(flat).max(axis=1)
        scales       = np.where(scales == 0, 1.0, scales) / 127.0
        q_flat       = np.clip(np.round(flat / scales[:, None]),
                               -127, 127).astype(np.int8)
        q            = np.moveaxis(q_flat.reshape(arr_ch_first.shape), 0, axis)
        return q, scales.astype(np.float32)

    scale = float(np.abs(arr).max() / 127.0 or 1.0)
    q     = np.clip(np.round(arr / scale), -127, 127).astype(np.int8)
    return q, np.float32(scale)


def quantize_bias(bias: np.ndarray, weight_scales, in_scale: float = 1.0
                  ) -> np.ndarray:
    """
    bias_int32 = round(bias_float / (in_scale * weight_scale))
    `weight_scales` may be scalar or per-channel.
    """
    q = np.round(bias / (in_scale * weight_scales)).astype(np.int64)
    q = np.clip(q, -2**31, 2**31 - 1).astype(np.int32)
    return q


def write_array(fh, arr: np.ndarray) -> int:
    """
    Write `arr` raw bytes, return the starting offset.
    """
    off = fh.tell()
    fh.write(arr.tobytes())
    return off


# ---------------------------------------------------------------------
#  Fixed-point helpers
# ---------------------------------------------------------------------
def to_q15(in_scale: float, w_scale: float, _out_shift: int) -> int:
    """
    Return Q0.15 multiplier as signed int16.
    Note: OUT_SHIFT is *not* folded in here; the kernel shifts by
    (15 + OUT_SHIFT) after the multiply.
    """
    alpha = in_scale * w_scale                # no 1<<out_shift term
    m = int(round(alpha * 32768))
    return max(-32768, min(32767, m))


def theor_shift(k: int) -> int:
    """ceil(log2(k*127)) – shift that avoids INT8 saturation."""
    return int(math.ceil(math.log2(max(1, k) * 127.0)))


def auto_fill_shifts(cfg: dict) -> None:
    """
    Fill cfg['shifts'] with theoretical values if missing or
    --auto-shift is requested.
    """
    if cfg.get("shifts") and not cfg.get("_force_auto", False):
        return
    dm, dff = cfg["DMODEL"], cfg["DFF"]
    hd      = dm // cfg["HEADS"]
    pd      = cfg.get("PATCH_DIM", dm)
    cfg["shifts"] = {
        "PE_SHIFT":    theor_shift(pd),
        "QKV_SHIFT":   theor_shift(dm),
        "ATT_SHIFT":   max(0, theor_shift(hd) - 4),   # softmax shrinks range
        "MHA_O_SHIFT": theor_shift(dm),
        "FFN_SHIFT1":  theor_shift(dm),
        "FFN_SHIFT2":  theor_shift(dff),
        "HEAD_SHIFT":  theor_shift(dm),
    }


# ---------------------------------------------------------------------
#  Header generator
# ---------------------------------------------------------------------
def gen_header(cfg: dict,
               w_ofs: Dict[str, int],
               sc_q15: Dict[str, int],
               arena: dict) -> str:
    lines = [
        "/* model.h — AUTOGENERATED; do not edit */",
        "#ifndef MODEL_H_",
        "#define MODEL_H_",
        "",
        "#include <stdint.h>",
        "",
    ]

    # Model dimensions
    for key in (
        "DMODEL", "DFF", "HEADS", "TOKENS",
        "PATCHES", "PATCH_DIM", "LAYERS", "OUT_DIM"
    ):
        lines.append(f"#define MODEL_{key:<11} {cfg[key]}")
    lines.append(f"#define MODEL_EPS_SHIFT {cfg['EPS_SHIFT']}\n")

    # Fixed-point shifts
    for name, val in cfg["shifts"].items():
        lines.append(f"#define {name:<14} {val}")
    lines.append("")

    # Arena layout
    lines.append(f"#define ARENA_BYTES      {arena['bytes']}")
    for name, off in arena["offsets"].items():
        lines.append(f"#define ARENA_OFF_{name:<6} {off}")
    lines.append("")

    # Weight/bias offsets (sorted for stability)
    for name in sorted(w_ofs):
        lines.append(f"#define OFF_{name:<13} {w_ofs[name]}")
    lines.append("")

    # Pre-computed Q0.15 scalers
    for name in sorted(sc_q15):
        lines.append(f"#define SC_{name:<11} {sc_q15[name]}")
    lines.append("")

    lines += [
        "extern const uint8_t  weights_bin[];",
        "",
        "#endif /* MODEL_H_ */",
        "",
    ]
    return "\n".join(lines)


# ---------------------------------------------------------------------
#  Main
# ---------------------------------------------------------------------
def main() -> None:
    ap = argparse.ArgumentParser("Pack Tiny-ViT ONNX → int8 blobs + header")
    ap.add_argument("--model", required=True, help="input .onnx file")
    ap.add_argument("--cfg",   required=True, help="YAML config")
    ap.add_argument("--out",   required=True, help="output directory")
    ap.add_argument("--auto-shift", action="store_true",
                    help="re-compute shifts ignoring YAML")
    args = ap.parse_args()

    out_dir = Path(args.out)
    out_dir.mkdir(parents=True, exist_ok=True)

    # --- YAML ----------------------------------------------------------------
    cfg = yaml.safe_load(Path(args.cfg).read_text())
    if args.auto_shift or not cfg.get("shifts"):
        cfg["_force_auto"] = args.auto_shift
        auto_fill_shifts(cfg)

    # --- ONNX ----------------------------------------------------------------
    model = onnx.load(args.model)
    inits = {t.name: t for t in model.graph.initializer}

    w_ofs: Dict[str, int] = {}
    sc_q15: Dict[str, int] = {}

    weights_path = out_dir / "weights.bin"
    with open(weights_path, "wb") as wfh:
        # ---------------------------------------------------------------------
        # Patch-embedding  (shape [DMODEL, PATCH_DIM] or conv O×I×kH×kW)
        # ---------------------------------------------------------------------
        w_pe = onnx.numpy_helper.to_array(inits["patch_embed.weight"])
        if w_pe.ndim == 4:                            # conv kernel
            w_pe = w_pe.reshape(w_pe.shape[0], -1)
        q_pe, s_pe = sym_int8_quant(w_pe, per_channel=True, axis=0)
        w_ofs["W_PE"] = write_array(wfh, q_pe)
        sc_q15["PE"]  = to_q15(1.0, float(s_pe.max()), cfg["shifts"]["PE_SHIFT"])

        b_pe = onnx.numpy_helper.to_array(inits["patch_embed.bias"])
        w_ofs["B_PE"] = write_array(wfh, quantize_bias(b_pe, s_pe))

        # ---------------------------------------------------------------------
        # Transformer blocks
        # ---------------------------------------------------------------------
        L   = cfg["LAYERS"]
        DM  = cfg["DMODEL"]

        for l in range(L):
            # ----- Fused QKV weight [3*DM, DM]
            wqkv = onnx.numpy_helper.to_array(
                inits[f"layers.{l}.attn.qkv.weight"]
            )
            wqkv = wqkv.reshape(3, DM, DM).transpose(0, 2, 1).reshape(-1, DM)
            q, s = sym_int8_quant(wqkv, per_channel=True, axis=0)
            w_ofs[f"W_QKV_L{l}"] = write_array(wfh, q)
            sc_q15[f"QKV_L{l}"]  = to_q15(
                1.0, float(s.max()), cfg["shifts"]["QKV_SHIFT"]
            )

            b = onnx.numpy_helper.to_array(inits[f"layers.{l}.attn.qkv.bias"])
            w_ofs[f"B_QKV_L{l}"] = write_array(wfh, quantize_bias(b, s))

            # ----- Output projection
            wo = onnx.numpy_helper.to_array(
                inits[f"layers.{l}.attn.proj.weight"]
            ).T
            q, s = sym_int8_quant(wo, per_channel=True, axis=0)
            w_ofs[f"W_O_L{l}"] = write_array(wfh, q)
            sc_q15[f"MHAO_L{l}"] = to_q15(
                1.0, float(s.max()), cfg["shifts"]["MHA_O_SHIFT"]
            )

            b = onnx.numpy_helper.to_array(inits[f"layers.{l}.attn.proj.bias"])
            w_ofs[f"B_O_L{l}"] = write_array(wfh, quantize_bias(b, s))

            # ----- FFN fc1
            w1 = onnx.numpy_helper.to_array(
                inits[f"layers.{l}.ffn.fc1.weight"]
            ).T
            q, s = sym_int8_quant(w1, per_channel=True, axis=0)
            w_ofs[f"W_FFN1_L{l}"] = write_array(wfh, q)
            sc_q15[f"FFN1_L{l}"] = to_q15(
                1.0, float(s.max()), cfg["shifts"]["FFN_SHIFT1"]
            )

            b = onnx.numpy_helper.to_array(inits[f"layers.{l}.ffn.fc1.bias"])
            w_ofs[f"B_FFN1_L{l}"] = write_array(wfh, quantize_bias(b, s))

            # ----- FFN fc2
            w2 = onnx.numpy_helper.to_array(
                inits[f"layers.{l}.ffn.fc2.weight"]
            ).T
            q, s = sym_int8_quant(w2, per_channel=True, axis=0)
            w_ofs[f"W_FFN2_L{l}"] = write_array(wfh, q)
            sc_q15[f"FFN2_L{l}"] = to_q15(
                1.0, float(s.max()), cfg["shifts"]["FFN_SHIFT2"]
            )

            b = onnx.numpy_helper.to_array(inits[f"layers.{l}.ffn.fc2.bias"])
            w_ofs[f"B_FFN2_L{l}"] = write_array(wfh, quantize_bias(b, s))

            # ----- LayerNorm γ/β
            for ln in ("ln1", "ln2"):
                g = onnx.numpy_helper.to_array(
                    inits[f"layers.{l}.{ln}.gamma"]
                ).astype(np.float32)
                b = onnx.numpy_helper.to_array(
                    inits[f"layers.{l}.{ln}.beta"]
                ).astype(np.float32)
                to_i32 = lambda x: np.round(x * 32768).clip(
                    -32768, 32767).astype(np.int32)
                w_ofs[f"G_{ln.upper()}_L{l}"] = write_array(wfh, to_i32(g))
                w_ofs[f"B_{ln.upper()}_L{l}"] = write_array(wfh, to_i32(b))

        # ---------------------------------------------------------------------
        # CLS token embedding
        # ---------------------------------------------------------------------
        cls = onnx.numpy_helper.to_array(inits["cls_token"]).astype(np.int8).flatten()
        w_ofs["CLS_EMB"] = write_array(wfh, cls)

        # ---------------------------------------------------------------------
        # Classifier head  [OUT_DIM, DMODEL]
        # ---------------------------------------------------------------------
        w_head = onnx.numpy_helper.to_array(inits["head.weight"])
        q, s   = sym_int8_quant(w_head, per_channel=True, axis=0)
        w_ofs["HEAD_W"] = write_array(wfh, q)
        sc_q15["HEAD"]  = to_q15(
            1.0, float(s.max()), cfg["shifts"]["HEAD_SHIFT"]
        )

        b_head = onnx.numpy_helper.to_array(inits["head.bias"])
        w_ofs["HEAD_B"] = write_array(wfh, quantize_bias(b_head, s))

    # -------------------------------------------------------------------------
    # Runtime arena layout (token-major ping-pong)
    # -------------------------------------------------------------------------
    tok, dm, dff = cfg["TOKENS"], cfg["DMODEL"], cfg["DFF"]
    arena = {
        "bytes": tok * dm * 5 + tok * dff,   # BUF0 BUF1 Q K V + TMP
        "offsets": {
            "BUF0": 0,
            "BUF1": tok * dm,
            "Q":    tok * dm * 2,
            "K":    tok * dm * 3,
            "V":    tok * dm * 4,
            "TMP":  tok * dm * 5,
        },
    }

    # -------------------------------------------------------------------------
    # Write header
    # -------------------------------------------------------------------------
    (out_dir / "model.h").write_text(gen_header(cfg, w_ofs, sc_q15, arena))

    kb_w = weights_path.stat().st_size / 1024
    print(f"✅  Generated weights.bin ({kb_w:.1f} KB) and model.h")

if __name__ == "__main__":
    main()
